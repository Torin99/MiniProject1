{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbf00d2",
   "metadata": {},
   "source": [
    "# <div align=\"center\">CP322-A Mini-Project 1: Machine Learning</div>\n",
    "## <div align=\"center\">Group 6</div>\n",
    "### <div align=\"center\">due on 15-Oct-2023 at 11:30 PM</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52edb19",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f51c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831ebc8",
   "metadata": {},
   "source": [
    "## Task 1: Acquire, preprocess, and analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d9eb8",
   "metadata": {},
   "source": [
    "1. Load the datasets into NumPy objects (i.e., arrays or matrices) in Python. Remember to convert the wine dataset\n",
    "to a binary task, as discussed above.\n",
    "2. Clean the data. Are there any missing or malformed features? Are there other data oddities that need to be\n",
    "dealt with? You should remove any examples with missing or malformed features and note this in your\n",
    "report. For categorical variables, you can use a one-hot encoding.\n",
    "3. Compute basic statistics on the data to understand it better. E.g., what are the distributions of the positive vs.\n",
    "negative classes, what are the distributions of some of the numerical features? what are the correlations between\n",
    "the features? how do the scatter plots of pair-wise features look like for some subset of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bcc588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                row = line.split(\",\")\n",
    "                if filename == \"adult.data\":\n",
    "                    # Convert non-numerical features to float\n",
    "                    age = float(row[0])\n",
    "                    fnlwgt = float(row[2])\n",
    "                    education_num = float(row[4])\n",
    "                    capital_gain = float(row[10])\n",
    "                    capital_loss = float(row[11])\n",
    "                    hours_per_week = float(row[12])\n",
    "                    # Combine the numerical features\n",
    "                    numerical_features = [age, fnlwgt, education_num, capital_gain, capital_loss, hours_per_week]\n",
    "                    # Append the numerical features\n",
    "                    data.append(numerical_features)\n",
    "                    label = row[-1]\n",
    "                    # Map the labels to binary values, e.g., '<=50K' to 0 and '>50K' to 1\n",
    "                    labels.append(0 if label == ' <=50K' else 1)\n",
    "                elif filename == \"Rice_Cammeo_Osmancik.arff.txt\":\n",
    "                    data.append([float(val) for val in row[:-1]])\n",
    "                    label = row[-1]\n",
    "                    labels.append(0 if label == 'Cammeo' else 1)\n",
    "                elif filename == \"agaricus-lepiota.data\":\n",
    "                    label = 0 if row[0] == 'e' else 1\n",
    "                    labels.append(label)\n",
    "\n",
    "                    # Define mappings for categorical values\n",
    "                    cap_shape_mapping = {'b': 0, 'c': 1, 'x': 2, 'f': 3, 'k': 4, 's': 5}\n",
    "                    cap_surface_mapping = {'f': 0, 'g': 1, 'y': 2, 's': 3}\n",
    "                    cap_color_mapping = {'n': 0, 'b': 1, 'c': 2, 'g': 3, 'r': 4, 'p': 5, 'u': 6, 'e': 7, 'w': 8, 'y': 9}\n",
    "                    bruises_mapping = {'t': 0, 'f': 1}\n",
    "                    odor_mapping = {'a': 0, 'l': 1, 'c': 2, 'y': 3, 'f': 4, 'm': 5, 'n': 6, 'p': 7, 's': 8}\n",
    "                    gill_attachment_mapping = {'a': 0, 'd': 1, 'f': 2, 'n': 3}\n",
    "                    gill_spacing_mapping = {'c': 0, 'w': 1, 'd': 2}\n",
    "                    gill_size_mapping = {'b': 0, 'n': 1}\n",
    "                    gill_color_mapping = {'k': 0, 'n': 1, 'b': 2, 'h': 3, 'g': 4, 'r': 5, 'o': 6, 'p': 7, 'u': 8, 'e': 9, 'w': 10, 'y': 11}\n",
    "                    stalk_shape_mapping = {'e': 0, 't': 1}\n",
    "                    stalk_root_mapping = {'b': 0, 'c': 1, 'u': 2, 'e': 3, 'z': 4, 'r': 5, '?': 6}\n",
    "                    stalk_surface_above_ring_mapping = {'f': 0, 'y': 1, 'k': 2, 's': 3}\n",
    "                    stalk_surface_below_ring_mapping = {'f': 0, 'y': 1, 'k': 2, 's': 3}\n",
    "                    stalk_color_above_ring_mapping = {'n': 0, 'b': 1, 'c': 2, 'g': 3, 'o': 4, 'p': 5, 'e': 6, 'w': 7, 'y': 8}\n",
    "                    stalk_color_below_ring_mapping = {'n': 0, 'b': 1, 'c': 2, 'g': 3, 'o': 4, 'p': 5, 'e': 6, 'w': 7, 'y': 8}\n",
    "                    veil_type_mapping = {'p': 0, 'u': 1}\n",
    "                    veil_color_mapping = {'n': 0, 'o': 1, 'w': 2, 'y': 3}\n",
    "                    ring_number_mapping = {'n': 0, 'o': 1, 't': 2}\n",
    "                    ring_type_mapping = {'c': 0, 'e': 1, 'f': 2, 'l': 3, 'n': 4, 'p': 5, 's': 6, 'z': 7}\n",
    "                    spore_print_color_mapping = {'k': 0, 'n': 1, 'b': 2, 'h': 3, 'r': 4, 'o': 5, 'u': 6, 'w': 7, 'y': 8}\n",
    "                    population_mapping = {'a': 0, 'c': 1, 'n': 2, 's': 3, 'v': 4, 'y': 5}\n",
    "                    habitat_mapping = {'g': 0, 'l': 1, 'm': 2, 'p': 3, 'u': 4, 'w': 5, 'd': 6}\n",
    "\n",
    "                    # Convert non-numerical features to float using the mappings\n",
    "                    encoded_features = [\n",
    "                        cap_shape_mapping[row[1]],\n",
    "                        cap_surface_mapping[row[2]],\n",
    "                        cap_color_mapping[row[3]],\n",
    "                        bruises_mapping[row[4]],\n",
    "                        odor_mapping[row[5]],\n",
    "                        gill_attachment_mapping[row[6]],\n",
    "                        gill_spacing_mapping[row[7]],\n",
    "                        gill_size_mapping[row[8]],\n",
    "                        gill_color_mapping[row[9]],\n",
    "                        stalk_shape_mapping[row[10]],\n",
    "                        stalk_root_mapping[row[11]],\n",
    "                        stalk_surface_above_ring_mapping[row[12]],\n",
    "                        stalk_surface_below_ring_mapping[row[13]],\n",
    "                        stalk_color_above_ring_mapping[row[14]],\n",
    "                        stalk_color_below_ring_mapping[row[15]],\n",
    "                        veil_type_mapping[row[16]],\n",
    "                        veil_color_mapping[row[17]],\n",
    "                        ring_number_mapping[row[18]],\n",
    "                        ring_type_mapping[row[19]],\n",
    "                        spore_print_color_mapping[row[20]],\n",
    "                        population_mapping[row[21]],\n",
    "                        habitat_mapping[row[22]]\n",
    "                    ]\n",
    "                    data.append(encoded_features) \n",
    "                        \n",
    "                else:\n",
    "                    data.append([float(val) for val in row[:-1]])\n",
    "                    label = row[-1]\n",
    "                    labels.append(0 if label == 'b' else 1)\n",
    "\n",
    "\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10493e",
   "metadata": {},
   "source": [
    "### Dataset 1 (Ionosphere): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c95110",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/ionosphere.data\"\n",
    "\n",
    "data,labels = readFile(filename)\n",
    "\n",
    "# Count the number of positive class instances\n",
    "positive_count = sum(1 for label in labels if label == 1)\n",
    "\n",
    "# Count the number of negative class instances\n",
    "negative_count = sum(1 for label in labels if label == 0)\n",
    "\n",
    "#what are the distributions of the positive vs. negative classes?\n",
    "print(\"Distribution of classes:\")\n",
    "print(\"Positive (g):\", positive_count)\n",
    "print(\"Negative (b):\", negative_count)\n",
    "\n",
    "print(\"\\nData:\")\n",
    "print(data,labels)\n",
    "\n",
    "#what are the distributions of some of the numerical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51515f",
   "metadata": {},
   "source": [
    "### Dataset 2 (Adult Data Set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6926886",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/adult.data\"\n",
    "\n",
    "data, labels = readFile(filename)\n",
    "\n",
    "# Count the number of positive class instances\n",
    "positive_count = sum(1 for label in labels if label == 1)\n",
    "\n",
    "# Count the number of negative class instances\n",
    "negative_count = sum(1 for label in labels if label == 0)\n",
    "\n",
    "print(\"Distribution of classes:\")\n",
    "print(\"Positive (>50):\", positive_count)\n",
    "print(\"Negative (<=50):\", negative_count)\n",
    "\n",
    "print(\"\\nData:\")\n",
    "print(data, labels)\n",
    "\n",
    "#what are the distributions of some of the numerical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447e5e8",
   "metadata": {},
   "source": [
    "### Dataset 3 (Rice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f28798",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Rice_Cammeo_Osmancik.arff.txt\"\n",
    "\n",
    "data, labels = readFile(filename)\n",
    "\n",
    "# Count the number of positive class instances\n",
    "positive_count = sum(1 for label in labels if label == 1)\n",
    "\n",
    "# Count the number of negative class instances\n",
    "negative_count = sum(1 for label in labels if label == 0)\n",
    "\n",
    "print(\"Distribution of classes:\")\n",
    "print(\"Positive (Cammeo):\", positive_count)\n",
    "print(\"Negative (Osmancik):\", negative_count)\n",
    "\n",
    "print(\"\\nData:\")\n",
    "print(data, labels)\n",
    "\n",
    "#what are the distributions of some of the numerical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375b029",
   "metadata": {},
   "source": [
    "### Dataset 4 (Mushroom):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"agaricus-lepiota.data\"\n",
    "data, labels = readFile(filename)\n",
    "\n",
    "# Count the number of positive class instances\n",
    "positive_count = sum(1 for label in labels if label == 1)\n",
    "\n",
    "# Count the number of negative class instances\n",
    "negative_count = sum(1 for label in labels if label == 0)\n",
    "\n",
    "print(\"Distribution of classes:\")\n",
    "print(\"Positive (Poisinous):\", positive_count)\n",
    "print(\"Negative (Edible):\", negative_count)\n",
    "\n",
    "print(\"\\nData:\")\n",
    "print(data, labels)\n",
    "\n",
    "#what are the distributions of some of the numerical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238ff8f",
   "metadata": {},
   "source": [
    "## Task 2: Implement the models\n",
    "\n",
    "#### 1. Implement logistic regression, and use (full batch) gradient descent for optimization.\n",
    "#### 2. Implement k-Nearest Neighbor (KNN), and find the best K.\n",
    "\n",
    "Implement both models as Python classes. You should use the constructor for the class to initialize the model\n",
    "parameters as attributes, as well as to define other important properties of the model.\n",
    "• Each of your models’ classes should have (at least) two functions:\n",
    "– Define a fit function, which takes the training data (i.e., x and y)—as well as other hyperparameters (e.g.,\n",
    "the learning rate and/or number of gradient descent iterations)—as input. This function should train your\n",
    "model by modifying the model parameters.\n",
    "– Define a predict function, which takes a set of input points (i.e., x) as input and outputs predictions (i.e.,\n",
    "yˆ) for these points. Note that for linear regression you need to convert probabilities to binary 0-1\n",
    "predictions by thresholding the output at 0.5!\n",
    "In addition to the model classes, you should also define functions evaluate_acc to evaluate the model accuracy.\n",
    "This function should take the true labels (i.e., y), and target labels (i.e., yˆ) as input, and it should output the accuracy\n",
    "score.\n",
    "• Lastly, you should implement a script to run k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958240fa",
   "metadata": {},
   "source": [
    "### Logistic Regression:\n",
    "Melissa\n",
    "Grant\n",
    "Yvonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a32049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.01\n",
    "        self.num_iterations = 1000\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Sigmoid function to convert values to probabilities between 0 and 1\n",
    "        return 1 / (1 + np.exp(-z)) #sigmoid(z) = 1 / ( 1 + e( - z ) )\n",
    "\n",
    "    def fit(self, data, labels): #training the logistic regression model\n",
    "        num_samples, num_features = data.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        converge=0.0001\n",
    "        converged = False\n",
    "        cost1 = 1\n",
    "        count = 0\n",
    "        while not converged and count<self.num_iterations:\n",
    "        # Gradient descent\n",
    "        #for i in range(self.num_iterations):\n",
    "            #Hypothesis Function\n",
    "            linear_model = np.dot(data, self.weights) + self.bias\n",
    "            predictions = self.sigmoid(linear_model)\n",
    "\n",
    "            # Compute gradients\n",
    "            #∂J/∂w = (1/m) * Σ[(h(x) - y) * x] , ∂J/∂b = (1/m) * Σ(h(x) - y)\n",
    "\n",
    "            dw = (1/num_samples) * np.dot((predictions - labels),data)\n",
    "            db = (1/num_samples) * np.sum(predictions - labels)\n",
    "\n",
    "            # Update the parameters in the opposite direction of the gradient\n",
    "            #w := w - α * ∂J/∂w  ,  b := b - α * ∂J/∂b\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            cost = 0   \n",
    "    \n",
    "            probabilities = self.sigmoid(linear_model)\n",
    "            cost = -1/num_samples * (np.dot(1 - labels, np.log(1 - probabilities + converge)) + np.dot(labels, np.log(probabilities + converge)))\n",
    "            if abs(cost1-cost)<=converge:\n",
    "                converged = True\n",
    "            cost1=cost\n",
    "            count+=1\n",
    "        return\n",
    "        \n",
    "        \n",
    "            \n",
    "    def predict(self, data):\n",
    "        #Hypothesis Function\n",
    "        linear_model = np.dot(data, self.weights) + self.bias\n",
    "        predictions = self.sigmoid(linear_model)\n",
    "        return [1 if p >= 0.5 else 0 for p in predictions]\n",
    "\n",
    "    def evaluate_acc(self, label_true, label_pred):\n",
    "        correct = np.sum(label_true == label_pred)\n",
    "        total = len(label_true)\n",
    "        return correct / total\n",
    "\n",
    "filename = \"adult.data\"\n",
    "\n",
    "data, labels = readFile(filename)\n",
    "\n",
    "# Combine features and labels\n",
    "data_with_labels = list(zip(data, labels))\n",
    "\n",
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "split_ratio = 0.7\n",
    "split_index = int(len(data_with_labels) * split_ratio)\n",
    "\n",
    "train_data, train_labels = zip(*data_with_labels[:split_index])\n",
    "test_data, test_labels = zip(*data_with_labels[split_index:])\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(np.array(train_data), np.array(train_labels))\n",
    "\n",
    "# Make predictions on the test data\n",
    "labels_pred = model.predict(np.array(test_data))\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = model.evaluate_acc(np.array(test_labels), labels_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "def k_fold (data, labels, k):\n",
    "    \n",
    "    accuracies = []\n",
    "    index_length = len(data)//k\n",
    "    counter = 0\n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    for i in range(k):\n",
    "        if i == k-1:\n",
    "           \n",
    "            data_testing_set = data[counter:]\n",
    "            data_training_set = data[:counter]\n",
    "            label_testing_set = labels[counter:]\n",
    "            label_training_set = labels[:counter]\n",
    "            \n",
    "        else:\n",
    "            data_testing_set = data[counter:index_length+counter]\n",
    "            label_testing_set = labels[counter:index_length+counter]\n",
    "            if counter == 0:\n",
    "                data_training_set = data[index_length+1:]\n",
    "                label_training_set = labels[index_length+1:]\n",
    "            else:\n",
    "                data_training_set = np.concatenate((data[0:counter] , data[index_length+counter+1:]))\n",
    "                label_training_set = np.concatenate((labels[0:counter] , labels[index_length+counter+1:]))\n",
    "        counter+=index_length\n",
    "\n",
    "        model.fit(np.array(data_training_set),np.array(label_training_set))\n",
    "        labels_pred = model.predict(np.array(data_testing_set))\n",
    "        accuracy = model.evaluate_acc(np.array(label_testing_set),np.array(labels_pred))\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "\n",
    "kfold = k_fold(data,labels, k = 4)\n",
    "print(\"kfold\",kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f6098",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor (KNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9cdf21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c70d4819",
   "metadata": {},
   "source": [
    "## Task 3: Run Experiments\n",
    "\n",
    "The goal of this project is to have you explore linear classification and compare different features and models. Use\n",
    "5-fold cross-validation to estimate performance in all of the experiments. Evaluate the performance using accuracy.\n",
    "You are welcome to perform any experiments and analyses you see fit (e.g., to compare different features), but at a\n",
    "minimum, you must complete the following experiments in the order stated below:\n",
    "\n",
    "#### 1. Compare the accuracy of k-nearest neighbor and logistic regression on the four datasets.\n",
    "\n",
    "#### 2. Test different k values for the k-nearest neighbor to find the best k-value by showing the accuracy plot. \n",
    "\n",
    "#### 3. Test different learning rates for gradient descent applied to logistic regression. Use a threshold for change in the value of the cost function as termination criteria and plot the accuracy on the train/validation set as a function of iterations of gradient descent.\n",
    "\n",
    "#### 4. Compare the accuracy of the two models as a function of the size of the dataset (by controlling the training size)\n",
    "\n",
    "Note: The above experiments are the minimum requirements that you must complete; however, this project is open-ended. For example, you might investigate different stopping criteria for gradient descent in logistic regression and develop an automated approach to select a good subset of features. You do not need to do all of these things, but you should demonstrate creativity, rigor, and an understanding of the course material in how you run your chosen experiments and how you report on them in your write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc70a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
