{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbf00d2",
   "metadata": {},
   "source": [
    "# <div align=\"center\">CP322-A Mini-Project 1: Machine Learning</div>\n",
    "## <div align=\"center\">Group 6</div>\n",
    "### <div align=\"center\">due on 15-Oct-2023 at 11:30 PM</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52edb19",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f51c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831ebc8",
   "metadata": {},
   "source": [
    "## Task 1: Acquire, preprocess, and analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d9eb8",
   "metadata": {},
   "source": [
    "1. Load the datasets into NumPy objects (i.e., arrays or matrices) in Python. Remember to convert the wine dataset\n",
    "to a binary task, as discussed above.\n",
    "2. Clean the data. Are there any missing or malformed features? Are there other data oddities that need to be\n",
    "dealt with? You should remove any examples with missing or malformed features and note this in your\n",
    "report. For categorical variables, you can use a one-hot encoding.\n",
    "3. Compute basic statistics on the data to understand it better. E.g., what are the distributions of the positive vs.\n",
    "negative classes, what are the distributions of some of the numerical features? what are the correlations between\n",
    "the features? how do the scatter plots of pair-wise features look like for some subset of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4bcc588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                row = line.split(\",\")\n",
    "                if filename==\"data/iris.data\":\n",
    "                    data.append([float(val) for val in row[:-1]])\n",
    "                    labels.append(row[-1])\n",
    "                else:\n",
    "                    data.append(row)\n",
    "\n",
    "    data = np.array(data)\n",
    "    if filename == \"data/iris.data\":\n",
    "        return data,labels\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10493e",
   "metadata": {},
   "source": [
    "### Dataset 1 (Ionosphere): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83c95110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes:\n",
      "Positive (g): 225\n",
      "Negative (b): 126\n",
      "\n",
      "Data:\n",
      "[['1' '0' '0.99539' ... '0.18641' '-0.45300' 'g']\n",
      " ['1' '0' '1' ... '-0.13738' '-0.02447' 'b']\n",
      " ['1' '0' '1' ... '0.56045' '-0.38238' 'g']\n",
      " ...\n",
      " ['1' '0' '0.94701' ... '0.92697' '-0.00577' 'g']\n",
      " ['1' '0' '0.90608' ... '0.87403' '-0.16243' 'g']\n",
      " ['1' '0' '0.84710' ... '0.85764' '-0.06151' 'g']]\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/ionosphere.data\"\n",
    "\n",
    "ionosphere_data = readFile(filename)\n",
    "X = ionosphere_data[:, :-1]  # All columns except the last one\n",
    "Y = (ionosphere_data[:, -1] == 'g').astype(bool)   # 1 if the class is 'g', 0 otherwise\n",
    "\n",
    "positive_count = np.sum(Y == 1)\n",
    "negative_count = np.sum(Y == 0)\n",
    "\n",
    "#what are the distributions of the positive vs. negative classes?\n",
    "print(\"Distribution of classes:\")\n",
    "print(\"Positive (g):\", positive_count)\n",
    "print(\"Negative (b):\", negative_count)\n",
    "\n",
    "print(\"\\nData:\")\n",
    "print(ionosphere_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51515f",
   "metadata": {},
   "source": [
    "### Dataset 2 (Adult Data Set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6926886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes:\n",
      "Positive (>50): 0\n",
      "Negative (<=50): 32561\n",
      "\n",
      "Data:\n",
      "[['1' '0' '0.99539' ... '0.18641' '-0.45300' 'g']\n",
      " ['1' '0' '1' ... '-0.13738' '-0.02447' 'b']\n",
      " ['1' '0' '1' ... '0.56045' '-0.38238' 'g']\n",
      " ...\n",
      " ['1' '0' '0.94701' ... '0.92697' '-0.00577' 'g']\n",
      " ['1' '0' '0.90608' ... '0.87403' '-0.16243' 'g']\n",
      " ['1' '0' '0.84710' ... '0.85764' '-0.06151' 'g']]\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/adult.data\"\n",
    "adult_data = readFile(filename)\n",
    "\n",
    "X = adult_data[:, :-1]  # All columns except the last one\n",
    "Y = (adult_data[:, -1] == '>50K').astype(int)   # 1 if the class is '>50', 0 otherwise\n",
    "\n",
    "#what are the distributions of the positive vs. negative classes?\n",
    "\n",
    "positive_count = np.sum(Y == 1)\n",
    "negative_count = np.sum(Y == 0)\n",
    "\n",
    "print(\"Distribution of classes:\")\n",
    "print(\"Positive (>50):\", positive_count)\n",
    "print(\"Negative (<=50):\", negative_count)\n",
    "\n",
    "print(\"\\nData:\")\n",
    "print(ionosphere_data)\n",
    "\n",
    "#what are the distributions of some of the numerical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447e5e8",
   "metadata": {},
   "source": [
    "### Dataset 3 (Iris Data Set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6f28798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/iris.data\"\n",
    "iris_data, iris_labels = readFile(filename)\n",
    "\n",
    "# X = np.array(iris_data)\n",
    "# Y = np.array(labels)\n",
    "\n",
    "print(iris_data)\n",
    "print(iris_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375b029",
   "metadata": {},
   "source": [
    "### Dataset 4 (Choice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca33eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7238ff8f",
   "metadata": {},
   "source": [
    "## Task 2: Implement the models\n",
    "\n",
    "#### 1. Implement logistic regression, and use (full batch) gradient descent for optimization.\n",
    "#### 2. Implement k-Nearest Neighbor (KNN), and find the best K.\n",
    "\n",
    "Implement both models as Python classes. You should use the constructor for the class to initialize the model\n",
    "parameters as attributes, as well as to define other important properties of the model.\n",
    "• Each of your models’ classes should have (at least) two functions:\n",
    "– Define a fit function, which takes the training data (i.e., x and y)—as well as other hyperparameters (e.g.,\n",
    "the learning rate and/or number of gradient descent iterations)—as input. This function should train your\n",
    "model by modifying the model parameters.\n",
    "– Define a predict function, which takes a set of input points (i.e., x) as input and outputs predictions (i.e.,\n",
    "yˆ) for these points. Note that for linear regression you need to convert probabilities to binary 0-1\n",
    "predictions by thresholding the output at 0.5!\n",
    "In addition to the model classes, you should also define functions evaluate_acc to evaluate the model accuracy.\n",
    "This function should take the true labels (i.e., y), and target labels (i.e., yˆ) as input, and it should output the accuracy\n",
    "score.\n",
    "• Lastly, you should implement a script to run k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958240fa",
   "metadata": {},
   "source": [
    "### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6a32049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.5\n",
    "        self.convergence = 0.01\n",
    "        return None\n",
    "    \n",
    "    def fit(self, x,y,alpha,iterations):\n",
    "        m = len(x)\n",
    "        w = 0\n",
    "        b = 0\n",
    "\n",
    "        while self.convergence < self.J(w,b,m,x,y)- self.mse:\n",
    "            tmp_w = w - alpha*self.J(w,b,m,x,y)\n",
    "            tmp_b = b - alpha*self.J(w,b,m,x,y)\n",
    "\n",
    "            w = tmp_w\n",
    "            b = tmp_b\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def predict(self, x):\n",
    "        return None\n",
    "\n",
    "    def evaluate_acc(self, y, y_ex):\n",
    "        return None\n",
    "\n",
    "    def mse(self, m, w, b, x, y):\n",
    "        val = 0\n",
    "\n",
    "        for i in range(m):\n",
    "            val += (b + (w*x[i]) - y[i])**2\n",
    "\n",
    "        return val\n",
    "\n",
    "    def J(self, w, b, m, x, y):\n",
    "        return (1/2*m)*self.mse(m,w,b,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f6098",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor (KNN):\n",
    "Riley and Torin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9cdf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) a new data point is input that we need to classify\n",
    "# 2) check the classification of the k nearest elements\n",
    "# 3) assunming we have 2 unique classifications (a,b). we take the classification of the dominant group\n",
    "# 4) if a tie exists take the class with the shortest distance from \n",
    "\n",
    "#to calculate distance we can use the Euclidean distance formula sqrt(sum i to N (x1_i — x2_i)²)\n",
    "\n",
    "def e_distance(vector1, vector2):\n",
    "    distance = 0\n",
    "    for i in range(len(vector1)):\n",
    "        squared = pow(vector1[i] - vector2[i],2)\n",
    "        distance += squared\n",
    "    distance = sqrt(distance)\n",
    "    return(distance)\n",
    "\n",
    "def knn(k, train_data, test_data):\n",
    "    dist = []\n",
    "    for row in train_data:\n",
    "        distance = e_distance(train_row, x_test)\n",
    "        heapq.heappush(dist, (-distance, train_row))\n",
    "        if len(dist) > k:\n",
    "            heapq.heappop(dist)\n",
    "            \n",
    "    k_neighbours = [(train_row, -distance) for distance, train_row in sorted(dist)]\n",
    "    return k_neighbours\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d4819",
   "metadata": {},
   "source": [
    "## Task 3: Run Experiments\n",
    "\n",
    "The goal of this project is to have you explore linear classification and compare different features and models. Use\n",
    "5-fold cross-validation to estimate performance in all of the experiments. Evaluate the performance using accuracy.\n",
    "You are welcome to perform any experiments and analyses you see fit (e.g., to compare different features), but at a\n",
    "minimum, you must complete the following experiments in the order stated below:\n",
    "\n",
    "#### 1. Compare the accuracy of k-nearest neighbor and logistic regression on the four datasets.\n",
    "\n",
    "#### 2. Test different k values for the k-nearest neighbor to find the best k-value by showing the accuracy plot. \n",
    "\n",
    "#### 3. Test different learning rates for gradient descent applied to logistic regression. Use a threshold for change in the value of the cost function as termination criteria and plot the accuracy on the train/validation set as a function of iterations of gradient descent.\n",
    "\n",
    "#### 4. Compare the accuracy of the two models as a function of the size of the dataset (by controlling the training size)\n",
    "\n",
    "Note: The above experiments are the minimum requirements that you must complete; however, this project is open-ended. For example, you might investigate different stopping criteria for gradient descent in logistic regression and develop an automated approach to select a good subset of features. You do not need to do all of these things, but you should demonstrate creativity, rigor, and an understanding of the course material in how you run your chosen experiments and how you report on them in your write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc70a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
